{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.testing import assert_array_almost_equal\n",
    "\n",
    "from pytorch_probing import Interceptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDecoderBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Block of a Transform Decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim:int, n_head:int, dropout_rate:float=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = torch.nn.MultiheadAttention(embed_dim, n_head, bias=False, batch_first=True)\n",
    "        self.dropout_attention = torch.nn.Dropout(dropout_rate)\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(embed_dim)\n",
    "        self.linear1 = torch.nn.Linear(embed_dim, 4*embed_dim)\n",
    "        self.dropout_linear1 = torch.nn.Dropout(dropout_rate)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(4*embed_dim, embed_dim)\n",
    "        self.dropout_linear2 = torch.nn.Dropout(dropout_rate)\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        #Masked Multi-Head Attention\n",
    "        attention_mask = torch.nn.Transformer.generate_square_subsequent_mask(x.shape[1])\n",
    "        y1, _ = self.attention(x, x, x, is_causal=True, need_weights=False, attn_mask=attention_mask)\n",
    "        y1 = self.dropout_attention(y1)\n",
    "        \n",
    "        #Add & Norm\n",
    "        y1 = x+y1\n",
    "        y1 = self.layer_norm1(y1)\n",
    "        \n",
    "        #Feed Forward\n",
    "        y2 = self.dropout_linear1(self.linear1(y1))\n",
    "        y2 = self.relu(y2)\n",
    "        y2 = self.dropout_linear2(self.linear2(y2))\n",
    "        \n",
    "        #Add & Norm\n",
    "        result = y1+y2\n",
    "        result = self.layer_norm2(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleModel(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, n_layers, n_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        blocks = [TransformDecoderBlock(embed_dim, n_head, 0.1) for _ in range(n_layers)]\n",
    "        self.decoder = torch.nn.Sequential(*blocks)\n",
    "\n",
    "        self.linear = torch.nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        self.embedding_output = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        self.embedding_output = x.detach().clone()\n",
    "\n",
    "        x = self.decoder(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def my_method(self):\n",
    "        return \"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 8\n",
    "vocab_size = 10\n",
    "n_layers = 2\n",
    "n_head = 2\n",
    "\n",
    "example_model = ExampleModel(embed_dim, vocab_size, n_layers, n_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExampleModel(\n",
       "  (embedding): Embedding(10, 8)\n",
       "  (decoder): Sequential(\n",
       "    (0): TransformDecoderBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "      )\n",
       "      (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "      (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "      (relu): ReLU()\n",
       "      (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformDecoderBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "      )\n",
       "      (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "      (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "      (relu): ReLU()\n",
       "      (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=8, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"decoder/0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 2\n",
    "test_sequence_size = 3\n",
    "\n",
    "inputs = torch.empty([2, test_sequence_size], dtype=int)\n",
    "inputs[:][0] = 0\n",
    "inputs[:][1] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExampleModel(\n",
       "  (embedding): Embedding(10, 8)\n",
       "  (decoder): Sequential(\n",
       "    (0): TransformDecoderBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "      )\n",
       "      (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "      (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "      (relu): ReLU()\n",
       "      (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformDecoderBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "      )\n",
       "      (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "      (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "      (relu): ReLU()\n",
       "      (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=8, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_outputs = example_model(inputs)\n",
    "original_outputs2 = example_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"decoder.0\", \"embedding\"]\n",
    "\n",
    "intercepted_model = Interceptor(example_model, paths, detach=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExampleModel(\n",
       "  (embedding): InterceptorLayer(\n",
       "    (_module): Embedding(10, 8)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): InterceptorLayer(\n",
       "      (_module): TransformDecoderBlock(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "        )\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "        (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "        (relu): ReLU()\n",
       "        (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "        (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformDecoderBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "      )\n",
       "      (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "      (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "      (relu): ReLU()\n",
       "      (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=8, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercepted_outputs = intercepted_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_array_almost_equal(original_outputs.detach().numpy(), \n",
    "                          original_outputs2.detach().numpy(), \n",
    "                          decimal=5)\n",
    "\n",
    "assert_array_almost_equal(original_outputs.detach().numpy(), \n",
    "                          intercepted_outputs.detach().numpy(), \n",
    "                          decimal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_array_almost_equal(example_model.embedding_output.detach().numpy(), \n",
    "                          intercepted_model.outputs[\"embedding\"].detach().numpy(), \n",
    "                          decimal=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoder.0': None, 'embedding': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intercepted_model.interceptor_clear()\n",
    "intercepted_model.outputs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intercepted_model.my_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(intercepted_model, Interceptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(intercepted_model, ExampleModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Interceptor(\n",
       "  (_module): ExampleModel(\n",
       "    (embedding): InterceptorLayer(\n",
       "      (_module): Embedding(10, 8)\n",
       "    )\n",
       "    (decoder): Sequential(\n",
       "      (0): InterceptorLayer(\n",
       "        (_module): TransformDecoderBlock(\n",
       "          (attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "          )\n",
       "          (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "          (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "          (relu): ReLU()\n",
       "          (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "          (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): TransformDecoderBlock(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "        )\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "        (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "        (relu): ReLU()\n",
       "        (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "        (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=8, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intercepted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercepted_model.a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_model = intercepted_model.reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_model.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"decoder.0\", \"WRONG_PATH\", \"embedding\"]\n",
    "\n",
    "#Assert raises\n",
    "try:\n",
    "    intercepted_model = Interceptor(example_model, paths, detach=False)\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "model_string = pprint.pformat(example_model)\n",
    "\n",
    "assert \"Interceptor\" not in model_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5432, -0.0260,  0.3923,  0.0831,  0.7893, -0.7662, -0.0886,\n",
       "          -0.5506,  0.8688,  0.2964],\n",
       "         [ 0.5432, -0.0260,  0.3923,  0.0831,  0.7893, -0.7662, -0.0886,\n",
       "          -0.5506,  0.8688,  0.2964],\n",
       "         [ 0.5432, -0.0260,  0.3923,  0.0831,  0.7893, -0.7662, -0.0886,\n",
       "          -0.5506,  0.8688,  0.2964]],\n",
       "\n",
       "        [[-0.5086, -0.3076, -0.2503, -0.7200, -0.3460,  0.3234,  0.7612,\n",
       "          -0.6162,  0.0605, -0.1657],\n",
       "         [-0.5086, -0.3076, -0.2503, -0.7200, -0.3460,  0.3234,  0.7612,\n",
       "          -0.6162,  0.0605, -0.1657],\n",
       "         [-0.5086, -0.3076, -0.2503, -0.7200, -0.3460,  0.3234,  0.7612,\n",
       "          -0.6162,  0.0605, -0.1657]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = [\"decoder.0\", \"embedding\"]\n",
    "\n",
    "intercepted_model = Interceptor(example_model, paths, detach=False)\n",
    "\n",
    "intercepted_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Interceptor(\n",
       "  (_module): ExampleModel(\n",
       "    (embedding): InterceptorLayer(\n",
       "      (_module): Embedding(10, 8)\n",
       "    )\n",
       "    (decoder): Sequential(\n",
       "      (0): InterceptorLayer(\n",
       "        (_module): TransformDecoderBlock(\n",
       "          (attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "          )\n",
       "          (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "          (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "          (relu): ReLU()\n",
       "          (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "          (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): TransformDecoderBlock(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "        )\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "        (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "        (relu): ReLU()\n",
       "        (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "        (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=8, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(intercepted_model, \"intercepted_model.pth\")\n",
    "\n",
    "intercepted_model2 = torch.load(\"intercepted_model.pth\")\n",
    "intercepted_model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercepted_model2 = torch.load(\"intercepted_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert intercepted_model2.outputs[\"decoder.0\"] is None\n",
    "assert intercepted_model2.outputs[\"embedding\"] is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove(\"intercepted_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExampleModel(\n",
       "  (embedding): Embedding(10, 8)\n",
       "  (decoder): Sequential(\n",
       "    (0): TransformDecoderBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "      )\n",
       "      (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "      (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "      (relu): ReLU()\n",
       "      (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformDecoderBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "      )\n",
       "      (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "      (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "      (relu): ReLU()\n",
       "      (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=8, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intercepted_model.reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interceptor(\n",
      "  (_module): ExampleModel(\n",
      "    (embedding): InterceptorLayer(\n",
      "      (_module): Embedding(10, 8)\n",
      "    )\n",
      "    (decoder): Sequential(\n",
      "      (0): InterceptorLayer(\n",
      "        (_module): TransformDecoderBlock(\n",
      "          (attention): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
      "          )\n",
      "          (dropout_attention): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "          (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
      "          (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
      "          (relu): ReLU()\n",
      "          (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
      "          (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
      "          (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (1): TransformDecoderBlock(\n",
      "        (attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
      "        )\n",
      "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "        (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
      "        (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
      "        (relu): ReLU()\n",
      "        (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
      "        (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
      "        (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=8, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "with Interceptor(example_model, paths) as intercepted_model:\n",
    "    intercepted_model(inputs)\n",
    "    print(intercepted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elton\\Documents\\GitHub\\pytorch-probing\\src\\pytorch_probing\\module_wrapper.py:37: UserWarning: Model was reduced. Not intercepting results\n",
      "  warnings.warn(\"Model was reduced. Not intercepting results\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5432, -0.0260,  0.3923,  0.0831,  0.7893, -0.7662, -0.0886,\n",
       "          -0.5506,  0.8688,  0.2964],\n",
       "         [ 0.5432, -0.0260,  0.3923,  0.0831,  0.7893, -0.7662, -0.0886,\n",
       "          -0.5506,  0.8688,  0.2964],\n",
       "         [ 0.5432, -0.0260,  0.3923,  0.0831,  0.7893, -0.7662, -0.0886,\n",
       "          -0.5506,  0.8688,  0.2964]],\n",
       "\n",
       "        [[-0.5086, -0.3076, -0.2503, -0.7200, -0.3460,  0.3234,  0.7612,\n",
       "          -0.6162,  0.0605, -0.1657],\n",
       "         [-0.5086, -0.3076, -0.2503, -0.7200, -0.3460,  0.3234,  0.7612,\n",
       "          -0.6162,  0.0605, -0.1657],\n",
       "         [-0.5086, -0.3076, -0.2503, -0.7200, -0.3460,  0.3234,  0.7612,\n",
       "          -0.6162,  0.0605, -0.1657]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intercepted_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoder.0': tensor([[[-0.1937, -0.3488,  1.5743,  0.4033, -0.7181, -1.5511, -0.5654,\n",
       "            1.3994],\n",
       "          [-0.1937, -0.3488,  1.5743,  0.4033, -0.7181, -1.5511, -0.5654,\n",
       "            1.3994],\n",
       "          [-0.1937, -0.3488,  1.5743,  0.4033, -0.7181, -1.5511, -0.5654,\n",
       "            1.3994]],\n",
       " \n",
       "         [[-0.1782, -0.6812,  1.5887,  0.9179, -0.7413,  0.6664, -1.7629,\n",
       "            0.1906],\n",
       "          [-0.1782, -0.6812,  1.5887,  0.9179, -0.7413,  0.6664, -1.7629,\n",
       "            0.1906],\n",
       "          [-0.1782, -0.6812,  1.5887,  0.9179, -0.7413,  0.6664, -1.7629,\n",
       "            0.1906]]]),\n",
       " 'embedding': tensor([[[-0.2290,  0.0151, -0.0612, -0.2276, -0.7952, -1.4815, -0.1022,\n",
       "            0.5629],\n",
       "          [-0.2290,  0.0151, -0.0612, -0.2276, -0.7952, -1.4815, -0.1022,\n",
       "            0.5629],\n",
       "          [-0.2290,  0.0151, -0.0612, -0.2276, -0.7952, -1.4815, -0.1022,\n",
       "            0.5629]],\n",
       " \n",
       "         [[-0.9286, -0.0813,  0.6004, -0.0322, -0.9358, -0.4568, -1.3712,\n",
       "           -0.7904],\n",
       "          [-0.9286, -0.0813,  0.6004, -0.0322, -0.9358, -0.4568, -1.3712,\n",
       "           -0.7904],\n",
       "          [-0.9286, -0.0813,  0.6004, -0.0322, -0.9358, -0.4568, -1.3712,\n",
       "           -0.7904]]])}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intercepted_model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
