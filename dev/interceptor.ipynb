{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.testing import assert_array_almost_equal\n",
    "\n",
    "from pytorch_probing import Interceptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformDecoderBlock(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Block of a Transform Decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim:int, n_head:int, dropout_rate:float=0.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = torch.nn.MultiheadAttention(embed_dim, n_head, bias=False, batch_first=True)\n",
    "        self.dropout_attention = torch.nn.Dropout(dropout_rate)\n",
    "        self.layer_norm1 = torch.nn.LayerNorm(embed_dim)\n",
    "        self.linear1 = torch.nn.Linear(embed_dim, 4*embed_dim)\n",
    "        self.dropout_linear1 = torch.nn.Dropout(dropout_rate)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(4*embed_dim, embed_dim)\n",
    "        self.dropout_linear2 = torch.nn.Dropout(dropout_rate)\n",
    "        self.layer_norm2 = torch.nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x:torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        #Masked Multi-Head Attention\n",
    "        attention_mask = torch.nn.Transformer.generate_square_subsequent_mask(x.shape[1])\n",
    "        y1, _ = self.attention(x, x, x, is_causal=True, need_weights=False, attn_mask=attention_mask)\n",
    "        y1 = self.dropout_attention(y1)\n",
    "        \n",
    "        #Add & Norm\n",
    "        y1 = x+y1\n",
    "        y1 = self.layer_norm1(y1)\n",
    "        \n",
    "        #Feed Forward\n",
    "        y2 = self.dropout_linear1(self.linear1(y1))\n",
    "        y2 = self.relu(y2)\n",
    "        y2 = self.dropout_linear2(self.linear2(y2))\n",
    "        \n",
    "        #Add & Norm\n",
    "        result = y1+y2\n",
    "        result = self.layer_norm2(result)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleModel(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, n_layers, n_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        blocks = [TransformDecoderBlock(embed_dim, n_head, 0.1) for _ in range(n_layers)]\n",
    "        self.decoder = torch.nn.Sequential(*blocks)\n",
    "\n",
    "        self.linear = torch.nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        self.embedding_output = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "\n",
    "        self.embedding_output = x.detach().clone()\n",
    "\n",
    "        x = self.decoder(x)\n",
    "        x = self.linear(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def my_method(self):\n",
    "        return \"A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 8\n",
    "vocab_size = 10\n",
    "n_layers = 2\n",
    "n_head = 2\n",
    "\n",
    "example_model = ExampleModel(embed_dim, vocab_size, n_layers, n_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExampleModel(\n",
       "  (embedding): Embedding(10, 8)\n",
       "  (decoder): Sequential(\n",
       "    (0): TransformDecoderBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "      )\n",
       "      (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "      (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "      (relu): ReLU()\n",
       "      (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformDecoderBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "      )\n",
       "      (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "      (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "      (relu): ReLU()\n",
       "      (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=8, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"decoder/0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 2\n",
    "test_sequence_size = 3\n",
    "\n",
    "inputs = torch.empty([2, test_sequence_size], dtype=int)\n",
    "inputs[:][0] = 0\n",
    "inputs[:][1] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExampleModel(\n",
       "  (embedding): Embedding(10, 8)\n",
       "  (decoder): Sequential(\n",
       "    (0): TransformDecoderBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "      )\n",
       "      (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "      (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "      (relu): ReLU()\n",
       "      (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): TransformDecoderBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "      )\n",
       "      (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "      (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "      (relu): ReLU()\n",
       "      (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=8, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_outputs = example_model(inputs)\n",
    "original_outputs2 = example_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"decoder.0\", \"embedding\"]\n",
    "\n",
    "intercepted_model = Interceptor(example_model, paths, detach=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExampleModel(\n",
       "  (embedding): InterceptorLayer(\n",
       "    (_module): Embedding(10, 8)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): InterceptorLayer(\n",
       "      (_module): TransformDecoderBlock(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "        )\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "        (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "        (relu): ReLU()\n",
       "        (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "        (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformDecoderBlock(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "      )\n",
       "      (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "      (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "      (relu): ReLU()\n",
       "      (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "      (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "      (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=8, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercepted_outputs = intercepted_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_array_almost_equal(original_outputs.detach().numpy(), \n",
    "                          original_outputs2.detach().numpy(), \n",
    "                          decimal=5)\n",
    "\n",
    "assert_array_almost_equal(original_outputs.detach().numpy(), \n",
    "                          intercepted_outputs.detach().numpy(), \n",
    "                          decimal=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_array_almost_equal(example_model.embedding_output.detach().numpy(), \n",
    "                          intercepted_model.outputs[\"embedding\"].detach().numpy(), \n",
    "                          decimal=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'decoder.0': None, 'embedding': None}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intercepted_model.interceptor_clear()\n",
    "intercepted_model.outputs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intercepted_model.my_method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(intercepted_model, Interceptor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(intercepted_model, ExampleModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Interceptor(\n",
       "  (_module): ExampleModel(\n",
       "    (embedding): InterceptorLayer(\n",
       "      (_module): Embedding(10, 8)\n",
       "    )\n",
       "    (decoder): Sequential(\n",
       "      (0): InterceptorLayer(\n",
       "        (_module): TransformDecoderBlock(\n",
       "          (attention): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "          )\n",
       "          (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "          (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "          (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "          (relu): ReLU()\n",
       "          (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "          (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (1): TransformDecoderBlock(\n",
       "        (attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=8, out_features=8, bias=False)\n",
       "        )\n",
       "        (dropout_attention): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm1): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "        (linear1): Linear(in_features=8, out_features=32, bias=True)\n",
       "        (dropout_linear1): Dropout(p=0.1, inplace=False)\n",
       "        (relu): ReLU()\n",
       "        (linear2): Linear(in_features=32, out_features=8, bias=True)\n",
       "        (dropout_linear2): Dropout(p=0.1, inplace=False)\n",
       "        (layer_norm2): LayerNorm((8,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (linear): Linear(in_features=8, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intercepted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercepted_model.a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_model = intercepted_model.reduce()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_model.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"decoder.0\", \"WRONG_PATH\", \"embedding\"]\n",
    "\n",
    "#Assert raises\n",
    "try:\n",
    "    intercepted_model = Interceptor(example_model, paths, detach=False)\n",
    "except ValueError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "model_string = pprint.pformat(example_model)\n",
    "\n",
    "assert \"Interceptor\" not in model_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1412,  0.1967,  0.5400,  0.6144,  0.1683, -0.1849, -0.4299,\n",
       "           0.6441,  0.5001,  0.3571],\n",
       "         [ 0.1412,  0.1967,  0.5400,  0.6144,  0.1683, -0.1849, -0.4299,\n",
       "           0.6441,  0.5001,  0.3571],\n",
       "         [ 0.1412,  0.1967,  0.5400,  0.6144,  0.1683, -0.1849, -0.4299,\n",
       "           0.6441,  0.5001,  0.3571]],\n",
       "\n",
       "        [[ 0.6534,  0.6210, -0.4276, -0.3513, -0.3976,  0.8657,  0.5498,\n",
       "           0.9061, -0.4704, -0.9327],\n",
       "         [ 0.6534,  0.6210, -0.4276, -0.3513, -0.3976,  0.8657,  0.5498,\n",
       "           0.9061, -0.4704, -0.9327],\n",
       "         [ 0.6534,  0.6210, -0.4276, -0.3513, -0.3976,  0.8657,  0.5498,\n",
       "           0.9061, -0.4704, -0.9327]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = [\"decoder.0\", \"embedding\"]\n",
    "\n",
    "intercepted_model = Interceptor(example_model, paths, detach=False)\n",
    "\n",
    "intercepted_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(intercepted_model, \"intercepted_model.pth\")\n",
    "\n",
    "intercepted_model2 = torch.load(\"intercepted_model\")\n",
    "intercepted_model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "intercepted_model2 = torch.load(\"intercepted_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert intercepted_model2.outputs[\"decoder.0\"] is None\n",
    "assert intercepted_model2.outputs[\"embedding\"] is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove(\"intercepted_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
